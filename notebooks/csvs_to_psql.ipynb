{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in CSV files to DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey patching pandas sql IO\n",
    "\n",
    "It turns out that there's an existing issue with pandas that limits its ability to perform insertions for multiple rows at a time. If I used the default one row at a time, then this operation would take far too long. See the link below for more about the existing issue.\n",
    "\n",
    "[Pandas to_sql issue link.](https://github.com/pandas-dev/pandas/issues/8953)\n",
    "\n",
    "Thanks to github user `nhockham` for suggesting the use of the monkey patch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.sql import SQLTable\n",
    "\n",
    "def _execute_insert(self, conn, keys, data_iter):\n",
    "    print('.', end='')\n",
    "    data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n",
    "    conn.execute(self.insert_statement().values(data))\n",
    "\n",
    "SQLTable._execute_insert = _execute_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass, getuser\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_files = [file for file in listdir('../data/') if file[-4:] == '.csv']\n",
    "\n",
    "\n",
    "u = input('Database user:')\n",
    "p = getpass('Input database password')\n",
    "engine_string = 'postgresql://{0}:{1}@handelstaccato.homenet.org:5432/king_county'.format(u, p)\n",
    "engine = create_engine(engine_string)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    table_name = csv_file.split('.')[0]\n",
    "    df = pd.read_csv(join('../data', csv_file), quotechar='\"', encoding='latin1')\n",
    "    df.to_sql(table_name, engine, schema='assessor_data', index=False, chunksize=1000)\n",
    "    # Just in case. I've been hurt too many times.\n",
    "    print('\"Finished\"', table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
