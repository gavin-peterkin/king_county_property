{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Residential Property Values\n",
    "\n",
    "Gavin Peterkin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Regardless of one's background or living situation, the housing market affects us all, but like most social and economic systems, it seems too complex to make any sense of. My aim in this project is to make use of the increasingly large public data sets made accessible by Seattle and King County to investigate the factors that influence a residential property's value over time. In effect, I seek to model and understand how features from 'number of bedrooms' to proximity to private schools and even to whether or not a property has a view of the Seattle skyline has any influence on that property's value and if so, by how much. Since tastes change over time, and I have access to sales data over a large period of time, I will also investigate how people's preferences are changing and extend that trend to the future to see what it _will_ mean for Seattle's booming housing market over the next couple of years.\n",
    "\n",
    "* Note: I was inspired partly by the [Kaggle home prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Workflow\n",
    "\n",
    "First, I downloaded raw csv files from the sources listed below.\n",
    "\n",
    "### Data Sources\n",
    "1. [King County Assessor Data](http://info.kingcounty.gov/Assessor/DataDownload/default.aspx): Has many features from tax assessor data including: recent sales prices, assessed values, slope of terrain, etc.\n",
    "2. [King County GIS Center](http://www.kingcounty.gov/services/gis.aspx): Has parcel geographic information systems (GIS) data including: parcel location and shape for multiple property types\n",
    "3. [Seattle Open Data](https://data.seattle.gov/): Has Latitudes and Longitudes for things like schools, libraries, parks, etc.\n",
    "\n",
    "After downloading the files, I developed scripts in the Bourne Again Shell (BASH) and/or python to automate (or partially automate) the process of cleaning the files and putting them into a PostgreSQL (PSQL) database on a remote server. There are significant speed limitations as a result of using a remote database, but it also allows for easy extensibility in the future. As more data become available, it can simply be dumped in, and the model can be re-run.\n",
    "\n",
    "Another benefit of PSQL is that there's an actively developed extension [PostGIS](http://www.postgis.net/) that allows for the creation of spatially/geographically enabled databases. This allows for relatively fast spatial computations that would be prohibitively slow if done in python.\n",
    "\n",
    "For more detail about how I pushed the data to the database and the issues I encountered along the way, see the notebook `csvs_to_psql` and the bash scripts `file_prep.bash` and `shp_processor.bash`. Shape files are an unusual file type. Luckily Postgis has functionality to ease the transition to PSQL. Still, I encountered a myriad of problems along the way mostly dealing with incorrect data types. I tried to fix these as they arose, but it was challenging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Finally, I had to do a lot of filtering, joining, and spatial operations in sql to get the final residential table. (In the sql directory, see `queries.sql`.) I generated a lot of the sql in the query file in the notebook `sql_generator`. This is where I create features like 'number of private schools in an x-meter radius'. This has been an iterative process. I'm continually updating the complete residential table as I discover issues or more useful features to add.\n",
    "\n",
    "As part of the feature engineering process, I have been reducing the dimensionality of several sets of features like the 'View' features. I used principal comonent analysis (PCA) for this process, but it's not clear if the results are actually better than keeping the original features intact. For one thing, it makes the results less interpretable.\n",
    "\n",
    "For categorical variables, namely the district, I created dummy variables. I think a better route in the future would be to create my own \"districts\" from latitude and longitude and try to cluster properties based on their assessed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis Findings\n",
    "\n",
    "Please see the [exploratory analysis notebook](./exploratory_analysis.ipynb) for an overview of some of the initial findings.\n",
    "\n",
    "Overall, there are about 50,000 properties with sale prices that range from zero to several million dollars.\n",
    "\n",
    "After filtering to recent sales that were determined to be 'good' (within a certain range from the assessed value and sold recently), there were around 4,000 properties to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection\n",
    "\n",
    "There are about 65 features in the original data. Based on the analysis in the [feature development notebook](./feature_development.ipynb), I hand selected features that seem to be correlated in some way with recent price.\n",
    "\n",
    "The district dummy features seem to be very predictive of home value, which aligns with one intuition. At the same time, why rely on the arbitrary division of district when we have information about the point locations of each property?\n",
    "\n",
    "The view features were dimensionall reduced using principle component analysis (PCA). As a result the view features lose some interpretability. It's also unclear if the view features are orthogonal to begin with in which case it doesn't really make much sense to reduce the dimensionality using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Please see the [modeling notebook](./modeling.ipynb) for more information.\n",
    "\n",
    "The relatively large number of features relative to the sample size of this data set requires that whatever model I use ignore useless features. Ridge and Lasso regressions perform L1 and L2 regularizations that can either diminish or eliminate features respectively that aren't useful. A Random Forest Regression (RFR) performed almost as well with several hundred estimators, but it's not clear how well it would perform on out-of-sample data.\n",
    "\n",
    "Regardless of what model I end up using, better features are an essential next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Conclusion\n",
    "\n",
    "* Don't reject the hypothesis that the housing features I ended up using influence sale price.\n",
    "* Where a house is located is probably the single most important factor in determining a home's value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps and areas for improvement\n",
    "\n",
    "### New features\n",
    "There are a lot of characteristics that this model simply isn't capturing. New features like a neighborhood \"livability\" score, affordability characteristics, employment center proximity, traffic, and store proximity are all very reasonable features to include in the next iteration of the model. There are also additional features within the sql tables that could be useful.\n",
    "\n",
    "Utilizing latitude and longitude and clustering properties based on neighboring property's characteristics may be a less arbitrary way of dividing up \"neighborhoods\" into more meaningful, price-predictive classes.\n",
    "\n",
    "* I chose early on to not use assessed value as a feature because it's _so_ highly correlated with sales price, but it could be something to revisit in later iterations\n",
    "\n",
    "### Improve feature selection\n",
    "Using L1 and L2 regularization techniques via Lasso and Ridge regressions and selecting features by hand seems a little primitive. It also does a poor job of distinguishing features that are highly correlated. Using random forests (RF) to rank features in importance could be another useful step in determining which features are useful especially as more and more features are added. I can also reduce the dimensionality of highly correlated features and engineer new features like \"shopping score\" or \"employment score\" that could be more predictive of value.\n",
    "\n",
    "### Model improvement\n",
    "None of the models are performing very well right now. Individually, they're all performing about the same--slightly better than null. I'll work on using ElasticNet and XGBoost next, and eventually I'll work on ensembling the best complimentary models.\n",
    "\n",
    "### Changing feature importance over time\n",
    "It would be fairly easy to model the importance of different features over time. The only difficulty would be in ensuring that I'm not using future features in the feature selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
